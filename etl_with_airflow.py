# -*- coding: utf-8 -*-
"""ETL_With_Airflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BowhEYiCtARJHdjm9_VEGkTt2tugdGQe
"""

from datetime import timedelta, datetime


from airflow import DAG 
from airflow.utils.dates import days_ago
from airflow.operators.dummy_operator import DummyOperator
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator
from airflow.providers.google.cloud.operators.bigquery import BigQueryCheckOperator
from airflow.contrib.operators.bigquery_operator import BigQueryOperator



GOOGLE_CONN_ID = "bigquery_default"
PROJECT_ID="vernal-hall-342913"
GS_PATH = "Files/"
BUCKET_NAME = 'disastersdata'
STAGING_DATASET = "staging"
DATASET = "Final"
LOCATION = "us-west1"

default_args = {
    'owner': 'Nitish Reddy Gaddam',
    'depends_on_past': False,
    'email_on_failure': ['gaddamnitishreddy@gmail.com'],
    'email_on_retry': False,
    'retries': 1,
    'start_date':  days_ago(2),
    'retry_delay': timedelta(minutes=5),
}

with DAG('ETL_With_Airflow', schedule_interval=timedelta(days=1), default_args=default_args) as dag:
    start_pipeline = DummyOperator(
        task_id = 'start_pipeline',
        dag = dag
        )


    load_staging_dataset = DummyOperator(
        task_id = 'load_staging_dataset',
        dag = dag
        )    
    
    load_declarations_data = GCSToBigQueryOperator(
        task_id = 'load_declarations_data',
        bucket = BUCKET_NAME,
        source_objects = ['Files/declarations_table_final.csv'],
        destination_project_dataset_table = f'{PROJECT_ID}:{STAGING_DATASET}.declarations_table',
        write_disposition='WRITE_TRUNCATE',
        source_format = 'csv',
        allow_quoted_newlines = 'true',
        skip_leading_rows = 1,
        schema_fields=[
        {'name': 'fema_declaration_string', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'disaster_number', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'declaration_type', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'declaration_date_up', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'fy_declared', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'declaration_title', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'disaster_closeout_date_up', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'fips', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'designated_area', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'declaration_request_numbeR', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'id', 'type': 'STRING', 'mode': 'REQUIRED'},
        {'name': 'last_refresh_up', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'state_mergekey', 'type': 'STRING', 'mode': 'NULLABLE'},
            ]
        )

    load_incidents_data = GCSToBigQueryOperator(
        task_id = 'load_incidents_data',
        bucket = BUCKET_NAME,
        source_objects = ['Files/incidents_final.csv'],
        destination_project_dataset_table = f'{PROJECT_ID}:{STAGING_DATASET}.incidents_table',
        write_disposition='WRITE_TRUNCATE',
        source_format = 'csv',
        allow_quoted_newlines = 'true',
        skip_leading_rows = 1,
        schema_fields=[
        {'name': 'incident_type', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'incident_begin_date_up', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'incident_end_date_up', 'type': 'DATE', 'mode': 'NULLABLE'},
        {'name': 'place_code', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'hash_id', 'type': 'STRING', 'mode': 'REQUIRED'},
            ]
        )

    load_programs_data = GCSToBigQueryOperator(
        task_id = 'load_programs_data',
        bucket = BUCKET_NAME,
        source_objects = ['Files/programs_final.csv'],
        destination_project_dataset_table = f'{PROJECT_ID}:{STAGING_DATASET}.programs_table',
        write_disposition='WRITE_TRUNCATE',
        source_format = 'csv',
        allow_quoted_newlines = 'true',
        skip_leading_rows = 1,
        schema_fields=[
        {'name': 'ih_program_declared', 'type': 'INTEGER', 'mode': 'NULLABLE'},
        {'name': 'ia_program_declared', 'type': 'INTEGER', 'mode': 'NULLABLE'},
        {'name': 'pa_program_declared', 'type': 'INTEGER', 'mode': 'NULLABLE'},
        {'name': 'hm_program_declared', 'type': 'INTEGER', 'mode': 'NULLABLE'},
        {'name': 'Programs_id', 'type': 'INTEGER', 'mode': 'REQUIRED'},
            ]
        )
    
    load_state_data = GCSToBigQueryOperator(
        task_id = 'load_state_data',
        bucket = BUCKET_NAME,
        source_objects = ['Files/states_final.csv'],
        destination_project_dataset_table = f'{PROJECT_ID}:{STAGING_DATASET}.states_table',
        write_disposition='WRITE_TRUNCATE',
        source_format = 'csv',
        allow_quoted_newlines = 'true',
        skip_leading_rows = 1,
        schema_fields=[
        {'name': 'State', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'Standard', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'state_mergekey', 'type': 'STRING', 'mode': 'REQUIRED'},
            ]
        )
    load_temperature_data = GCSToBigQueryOperator(
        task_id = 'load_temperature_data',
        bucket = BUCKET_NAME,
        source_objects = ['Files/temperature_final.csv'],
        destination_project_dataset_table = f'{PROJECT_ID}:{STAGING_DATASET}.temperature_table',
        write_disposition='WRITE_TRUNCATE',
        source_format = 'csv',
        allow_quoted_newlines = 'true',
        skip_leading_rows = 1,
        schema_fields=[
        {'name': 'temp_id', 'type': 'INTEGER', 'mode': 'REQUIRED'},
        {'name': 'Year', 'type': 'INTEGER', 'mode': 'NULLABLE'},
        {'name': 'State', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'Avg_Temperature', 'type': 'FLOAT', 'mode': 'NULLABLE'},
        {'name': 'state_mergekey', 'type': 'STRING', 'mode': 'NULLABLE'},
            ]
        )
    load_disaster_data = GCSToBigQueryOperator(
        task_id = 'load_disaster_data',
        bucket = BUCKET_NAME,
        source_objects = ['Files/disaster_final.csv'],
        destination_project_dataset_table = f'{PROJECT_ID}:{STAGING_DATASET}.disaster_table',
        write_disposition='WRITE_TRUNCATE',
        source_format = 'csv',
        allow_quoted_newlines = 'true',
        skip_leading_rows = 1,
        schema_fields=[
        {'name': 'id', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'hash_id', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'p_id', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'disaster_ID', 'type': 'INTEGER', 'mode': 'REQUIRED'},
            ]
        )     
    
    check_declarations = BigQueryCheckOperator(
        task_id = 'check_declarations',
        use_legacy_sql=False,
        location = LOCATION,
        sql = f'SELECT count(*) FROM `{PROJECT_ID}.{STAGING_DATASET}.declarations_table`'
        )

    check_incidents = BigQueryCheckOperator(
        task_id = 'check_incidents',
        use_legacy_sql=False,
        location = LOCATION,
        sql = f'SELECT count(*) FROM `{PROJECT_ID}.{STAGING_DATASET}.incidents_table`'
        )

    check_programs = BigQueryCheckOperator(
        task_id = 'check_programs',
        use_legacy_sql=False,
        location = LOCATION,
        sql = f'SELECT count(*) FROM `{PROJECT_ID}.{STAGING_DATASET}.programs_table`'
        ) 

    check_state = BigQueryCheckOperator(
        task_id = 'check_state',
        use_legacy_sql=False,
        location = LOCATION,
        sql = f'SELECT count(*) FROM `{PROJECT_ID}.{STAGING_DATASET}.states_table`'
        )
    check_temperature = BigQueryCheckOperator(
        task_id = 'check_temperature',
        use_legacy_sql=False,
        location = LOCATION,
        sql = f'SELECT count(*) FROM `{PROJECT_ID}.{STAGING_DATASET}.temperature_table`'
        )
    check_disaster = BigQueryCheckOperator(
        task_id = 'check_disaster',
        use_legacy_sql=False,
        location = LOCATION,
        sql = f'SELECT count(*) FROM `{PROJECT_ID}.{STAGING_DATASET}.disaster_table`'
        )
    create_D_Table = DummyOperator(
        task_id = 'Create_D_Table',
        dag = dag
        )

    create_declarations_table = BigQueryOperator(
        task_id = 'create_declarations_table',
        use_legacy_sql = False,
        location = LOCATION,
        sql = './sql/declarations_table.sql'
        )

    create_incidents_table = BigQueryOperator(
        task_id = 'create_incidents_table',
        use_legacy_sql = False,
        location = LOCATION,
        sql = './sql/incidents_table.sql'
        )   

    create_programs_table = BigQueryOperator(
        task_id = 'create_programs_table',
        use_legacy_sql = False,
        location = LOCATION,
        sql = './sql/programs_table.sql'
        )

    create_states_table = BigQueryOperator(
        task_id = 'create_states_table',
        use_legacy_sql = False,
        location = LOCATION,
        sql = './sql/states_table.sql'
        )
    create_temperature_table = BigQueryOperator(
        task_id = 'create_temperature_table',
        use_legacy_sql = False,
        location = LOCATION,
        sql = './sql/temperature_table.sql'
        )

    create_disaster_table = BigQueryOperator(
        task_id = 'create_disaster_table',
        use_legacy_sql = False,
        location = LOCATION,
        sql = './sql/disaster_table.sql'
        )

    create_fact_table = BigQueryOperator(
        task_id = 'create_fact_table',
        use_legacy_sql = False,
        location = LOCATION,
        sql = './sql/fact_table.sql'
        )

    check_fact_table = BigQueryCheckOperator(
        task_id = 'check_fact_table',
        use_legacy_sql=False,
        location = LOCATION,
        sql = f'SELECT count(*) FROM `{PROJECT_ID}.{DATASET}.fact_table`'
    )

    finish_pipeline = DummyOperator(
        task_id = 'finish_pipeline',
        dag = dag
        ) 
start_pipeline >> load_staging_dataset

load_staging_dataset >> [load_declarations_data, load_incidents_data, load_programs_data, load_state_data,load_temperature_data,load_disaster_data]

load_declarations_data >> check_declarations
load_incidents_data >> check_incidents
load_programs_data >> check_programs
load_state_data >> check_state
load_temperature_data >> check_temperature
load_disaster_data >> check_disaster

[check_declarations, check_incidents, check_programs, check_state, check_temperature, check_temperature, check_disaster] >> create_D_Table

create_D_Table >> [create_declarations_table, create_incidents_table, create_programs_table, create_states_table, create_temperature_table, create_disaster_table]

[create_declarations_table, create_incidents_table, create_programs_table, create_states_table,create_temperature_table,create_disaster_table] >> create_fact_table

create_fact_table >> check_fact_table >> finish_pipeline